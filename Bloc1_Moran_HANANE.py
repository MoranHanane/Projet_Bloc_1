import requests
import time
import xml.etree.ElementTree as ET
import json
import re

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

import pandas as pd
from datetime import datetime
import pymongo
import os
import mysql.connector
from dotenv import load_dotenv 


# I. Extraction des donn√©es
# I.1 interrogation de l'API et extraction des URL 


def extract_urls_from_xml(xml_data):
    """Cette fonction extrait les URLs des notices d√©taill√©es de la r√©ponse XML de Gallica et les stocke dans la liste URL"""
    urls = []
    root = ET.fromstring(xml_data)
    
    # L'√©l√©ment <record> contient l'URL de la notice d√©taill√©e
    for record in root.findall(".//{http://www.loc.gov/zing/srw/}record"):
        url_elem = record.find(".//{http://purl.org/dc/elements/1.1/}identifier")
        if url_elem is not None and "gallica.bnf.fr" in url_elem.text:
            urls.append(url_elem.text)

    return urls

# application de la fonction √† l'API de recherche
BASE_URL = "https://gallica.bnf.fr/SRU"
QUERY = '(dc.type all "fascicule") and (ocr.quality all "Texte disponible")'  
MAX_RECORDS = 50      # Limite impos√©e par l'API
TOTAL_RESULTS = 5000  # Nombre d'URL que je souhaite extraire
start_record = 0
results = []

while len(results) < TOTAL_RESULTS:
    params = {
        "operation": "searchRetrieve",
        "version": "1.2",
        "startRecord": start_record,
        "maximumRecords": MAX_RECORDS,
        "collapsing": "true",
        "exactSearch": "false",
        "query": QUERY
    }


    response = requests.get(BASE_URL, params=params)
    if response.status_code == 200:
        data = response.text  # R√©ponse en XML
        # Extraction des URLs des notices d√©taill√©es √† partir du XML
        urls = [url for url in extract_urls_from_xml(data)] 
        results.extend(urls)
        start_record += MAX_RECORDS
        time.sleep(0.5)  # Pause pour √©viter le blocage par l'API
    else:
        print(f"Erreur {response.status_code}")
        break

print(f"Nombre total de r√©sultats r√©cup√©r√©s : {len(results)}")



#I. 2 scapping des m√©tadonn√©es √† partir des URLs

# Configuration de Selenium
chrome_options = Options()
chrome_options.add_argument("--headless")  # headless = mode sans interface graphique
driver = webdriver.Chrome( options=chrome_options)
wait = WebDriverWait(driver, 10)


def get_metadata_from_notice(url):
    """Cette fonction permet de r√©cup√©rer des m√©tadonn√©es d'une revue Gallica apr√®s avoir cliqu√© sur le dropdown."""
    try:
        driver.set_page_load_timeout(15)  # Timeout √† 15 secondes
        driver.get(url)
    except Exception as e:
        print(f"‚è≥ Timeout d√©pass√© pour {url}, passage √† l'URL suivante...")
        return None

    start_time = time.time()  # D√©marrage du chrono

    try:
        print(f" {url} - V√©rification du bouton 'Informations d√©taill√©es'...")
        details = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div#moreInfosRegion")))
        details.click()
        print(" Dropdown cliqu√© !")

        print(" Attente du chargement des m√©tadonn√©es...")
        metadata_section = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "dl.noticeDetailsArea")))
        time.sleep(1)  # Pause suppl√©mentaire

        
        # V√©rifie si on r√©cup√®re bien toutes les cl√©s <dt>
        titles = metadata_section.find_elements(By.XPATH, "./dt")
        print(f" {len(titles)} m√©tadonn√©es trouv√©es sur {url}")

        data = {"url": url}

        for title in titles:
            key = title.text.strip().replace("\n", " ")  # Nettoyage cl√©

            if key:  # Filtre des cl√©s vides
                try:
                    content_elements = title.find_elements(By.XPATH, "./following-sibling::dd")
                    print(f" Cl√© d√©tect√©e : {key} - {len(content_elements)} √©l√©ments dd trouv√©s")

                    content = "; ".join([c.text.strip() for c in content_elements if c.text.strip()])
                    data[key] = content if content else "Valeur vide"
                except Exception:
                    print(f" M√©tadonn√©e sans valeur pour {key}")
                    data[key] = "Valeur manquante"
            else:
                print(f" Cl√© vide ignor√©e pour {url}")  # Nouvelle alerte

        elapsed_time = time.time() - start_time
        print(f" Donn√©es r√©cup√©r√©es en {elapsed_time:.2f} secondes pour {url}")

        return data

    except Exception as e:
        print(f"ATTENTION! Erreur sur {url} : {e}")
        time.sleep(5)
        return None



# Chargement de 50 URLs de test
with open("urls_gallica.json", "r") as f:
    urls = json.load(f)

urls = urls[:50]  # S√©lection de 50 URLs pour le test



# Scrapping de toutes les URL et stokage des r√©sultats
metadata_list = []
error_log = []

for i, url in enumerate(urls, start=1):
    print(f"üîπ {i}/{len(urls)} - Scraping {url}")
    metadata = get_metadata_from_notice(url)

    if metadata:
        metadata_list.append(metadata)
    else:
        error_log.append(url)

    time.sleep(0.5)  # Pause pour √©viter le blocage

# Sauvegarde en JSON
with open("metadata_gallica_test.json", "w", encoding="utf-8") as f:
    json.dump(metadata_list, f, indent=4, ensure_ascii=False)

# Sauvegarde en CSV
df = pd.DataFrame(metadata_list)
df.to_csv("metadata_gallica_test.csv", index=False, encoding="utf-8")

# Sauvegarde des erreurs
with open("erreurs_urls_test.txt", "w") as f:
    for url in error_log:
        f.write(url + "\n")

# Fermeture de Selenium
driver.quit()

print(f" Test termin√© ! {len(metadata_list)} r√©sultats enregistr√©s.")
print(f"ATTENTION! {len(error_log)} erreurs enregistr√©es dans erreurs_urls_test.txt")



# II Construction du Dataframe et nettoyage des donn√©es


# **Construction du DataFrame**
with open("metadata_gallica.json", "r", encoding="utf-8") as f:
    raw_data = json.load(f)

print(f"Nombre d'entr√©es : {len(raw_data)}")

all_keys = sorted(set().union(*(entry.keys() for entry in raw_data)))
df = pd.DataFrame([{key: entry.get(key, "") for key in all_keys} for entry in raw_data])

df.to_json("metadata_gallica_clean.json", orient="records", indent=4, force_ascii=False)
df.to_csv("metadata_gallica_clean.csv", index=False, encoding="utf-8")

print(f"Export termin√© ! {df.shape[0]} lignes et {df.shape[1]} colonnes g√©n√©r√©es.")



# II. 1. v√©rification et effacement des doublons
df.duplicated()
df.drop_duplicates(inplace=True)


# II. 2. Formatage des donn√©es pour chaque colonne


df["Conservation num√©rique :"] = df["Conservation num√©rique :"].apply(
    lambda x: "Biblioth√®que nationale de France" if x != "" and x != "Biblioth√®que nationale de France" else x
)
df["Auteur :"] = df["Auteur :"].str.split(". Auteur du texte").str[0]
# str.split(". Auteur du texte") : Cette m√©thode divise chaque cha√Æne de la colonne "Auteur :" en deux parties, en utilisant ". Auteur du texte" comme s√©parateur.
# .str[0] : Apr√®s la division, nous s√©lectionnons la premi√®re partie (celle √† gauche du s√©parateur).

df["Contributeur :"] = df["Contributeur :"].str.extract(r'^([^.(]+(?:\s*\([^)]*\))?)')
# Utilisation d'une regex pour extraire le texte avant le premier point de chaque ligne, en ignorant d'√©ventuels points dans les parenth√®ses (ann√©es de naissance et mort)
print(df["Contributeur :"].value_counts())


df["Date d'√©dition :"] = df["Date d'√©dition :"].str.split("-").str[0]
# Ceci permet d'extraire le texte √† gauche du premier tiret "-"

df["Date d'√©dition :"] = pd.to_numeric(df["Date d'√©dition :"], errors="coerce")
# Ceci transforme le texte en valeur num√©rique

# transformation en entiers --> Les NaNs sont remplac√©s par la valeur "2040"
df["Date d'√©dition :"] = df["Date d'√©dition :"].fillna(2040).astype(int)

df["Date de mise en ligne :"] = pd.to_datetime(
    df["Date de mise en ligne :"], format="%d/%m/%Y"
).dt.strftime("%Y-%m-%d")
# Ceci permet de convertir la colonne "Date de mise en ligne :" au format YYYY-MM-DD

df["Langue :"] = df["Langue :"].str.split(";").str[0].str.lower()
# Extraction du texte √† gauche du point-virgule ";" et suppression des majuscules




# GESTION DE LA SERIE "SOURCE"

# Fonction pour extraire ce qui est APR√àS la premi√®re ponctuation
def extract_after_punctuation(text):
    """Cette fonction permet d'extraire le texte apr√®s le premier caract√®re de ponctuation rencontr√©"""
    if pd.isna(text):  # G√©rer les NaN
        return ""
    
    match = re.search(r"[,:;.-]\s*(.*)", text)  # Capture apr√®s la ponctuation
    return match.group(1) if match else ""  

# Fonction pour extraire ce qui est AVANT la premi√®re ponctuation
def extract_before_punctuation(text):
    """Cette fonction permet d'extraire le texte avant le premier caract√®re de ponctuation rencontr√©"""
    if pd.isna(text):  # G√©rer les NaN
        return ""

    match = re.search(r"^(.*?)[,:;.-]", text)  # Capture avant la ponctuation
    return match.group(1).strip() if match else text  # Si pas de ponctuation, garder tout


# Application de la fonction sur la colonne "Source :"

df["Source (d√©tail)"] = df["Source :"].apply(extract_after_punctuation)  # Partie apr√®s la ponctuation
df["Source :"] = df["Source :"].apply(extract_before_punctuation)  # Partie avant la ponctuation

# Localisation de la position de la colonne "Source :"
source_col_index = df.columns.get_loc("Source :")

# Ins√©rtion de la colonne "Source (d√©tail)" juste apr√®s "Source :"
df.insert(source_col_index + 1, "Source (d√©tail)", df.pop("Source (d√©tail)"))


# Gestion de la s√©rie "Sujet"

def extraction_sujet(text):
    """Cette fonction permet d'extraire le sujet en fonction du pattern que j'ai d√©fini selon 3 r√®gles"""
    if pd.isna(text):  # Gestion des NaN
        return text
    
    # Condition 1 : Extraction √† gauche de " Relancer" (espace inclus)
    if " Relancer" in text:
        return text.split(" Relancer")[0].strip()
    
    # Condition 2 : Extraction √† gauche de " --" (espace inclus)
    if " --" in text:
        return text.split(" --")[0].strip()
    
    # Condition 3 : Si aucune des conditions n'est remplie, retourner le texte original
    return text

# Application de la fonction √† la colonne "Sujet :"
df["Sujet :"] = df["Sujet :"].apply(extraction_sujet)



# Gestion de la s√©rie "Editeur"

def extract_lieu(text):
    """ Extrait le lieu d'√©dition s'il est entre parenth√®ses ou apr√®s 'A' et avant la premi√®re virgule """
    if pd.isna(text) or text.strip() == "":
        return ""

    # Cas 1 : Lieu dans des parenth√®ses "(Lieu)"
    match = re.search(r"\(([^)]+)\)", text)
    if match:
        return match.group(1).strip()

    # Cas 2 : Lieu apr√®s "A " et avant la premi√®re virgule
    match = re.search(r"A ([^,]+),", text)
    if match:
        return match.group(1).strip()

    return ""

def extract_editeur(text):
    """ Extrait le nom de l'√©diteur en fonction de la structure du texte """
    if pd.isna(text) or text.strip() == "":
        return ""

    # Cas 1 : √âditeur avant la premi√®re parenth√®se "(Lieu)"
    match = re.search(r"^(.+?)\s*\(", text)
    if match:
        return match.group(1).strip()

    # Cas 2 : √âditeur entre la premi√®re et la deuxi√®me virgule apr√®s "A"
    match = re.search(r"A [^,]+, ([^,]+),", text)
    if match:
        return match.group(1).strip()

    return text.split(";")[0].strip()  # Par d√©faut, prendre avant le premier ';'

def extract_details(text):
    """ Extrait les d√©tails en supprimant les infos d√©j√† extraites et les liens """
    if pd.isna(text) or text.strip() == "":
        return ""

    # Supprimer les liens
    cleaned_text = re.sub(r"\| Liens?: .*", "", text)

    # Supprimer le lieu et l'√©diteur si extraits
    lieu = extract_lieu(text)
    editeur = extract_editeur(text)

    if lieu:
        cleaned_text = cleaned_text.replace(f"({lieu})", "").strip()
    if editeur:
        cleaned_text = cleaned_text.replace(editeur, "").strip()

    return cleaned_text

# Application des fonctions
df["Lieu"] = df["√âditeur :"].apply(extract_lieu)
df["√âditeur"] = df["√âditeur :"].apply(extract_editeur)
df["√âditeur (d√©tails)"] = df["√âditeur :"].apply(extract_details)



# export des versions finales du DF nettoy√© en local
df.to_json("metadata_gallica_cleanV2.json", orient="records", indent=4, force_ascii=False)
df.to_csv("metadata_gallica_cleanV2.csv", index=False, encoding="utf-8")

# III  Cr√©ation et remplissage des bases de donn√©es

load_dotenv() # chargement des variables d'environnement 

# Connexion √† MySQL
mysql_conn = mysql.connector.connect(
    host="localhost",
    user=os.getenv("DB_USER"),
    password=os.getenv("DB_PASSWORD"),
    database="Revues_numerisees"
)

mysql_cursor = mysql_conn.cursor()


# Connexion MongoDB via les variables d'environnement
mongo_client = pymongo.MongoClient(os.getenv("MONGO_URI"))  
mongo_db = mongo_client[os.getenv("MONGO_DB")]
notices_collection = mongo_db[os.getenv("MONGO_COLLECTION")]


# Chargement du dataframe
df = pd.read_csv("metadata_gallica_cleanV2.csv")


# Remplacement des NaN par des None dans le DataFrame  --> En effet, MySQL ne sait pas g√©rer les Nans mais "None" est g√©r√© par les deux: √©quivaut √† "Null" en SQL et "Nan" en Python
df = df.where(pd.notna(df), None)


# Troncature des colonnes VARCHAR avec un slicing √† 254 caract√®res   --> Permet de limiter la data ins√©r√©e en nombre de charact√®res et de rester en dessous de la limite de Varchar (255) 
varchar_columns = ["Titre :", "Contributeur :", "Langue :", "Identifiant :", "Source :", 
                   "Conservation num√©rique :", "Date d'√©dition :", "url"]

for col in varchar_columns:
    if col in df.columns:
        df[col] = df[col].apply(lambda x: x[:254] if isinstance(x, str) else x)



# Insertion des donn√©es SQL
for _, row in df.iterrows():
    # Insertion dans la table Revue
    sql_insert_revue = """
    INSERT INTO Revue (Titre, Contributeur, Langue, Identifiant, Source, Date_de_mise_en_ligne, 
                       Conservation_numerique, Date_d_edition, URL) 
    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
    """
    mysql_cursor.execute(sql_insert_revue, (
        row["Titre :"], row["Contributeur :"], row["Langue :"], row["Identifiant :"], 
        row["Source :"], row["Date de mise en ligne :"], row["Conservation num√©rique :"], 
        row["Date d'√©dition :"], row["url"]
    ))

    mysql_conn.commit()

    # R√©cup√©rer l'ID de la revue ins√©r√©e
    mysql_cursor.execute("SELECT LAST_INSERT_ID()")
    id_titre = mysql_cursor.fetchone()[0]

    # Insertion des Auteurs
    sql_insert_auteur = "INSERT INTO Auteur (Auteur) VALUES (%s)"
    if pd.notna(row["Auteur :"]):  # V√©rifier si la colonne n'est pas vide
        mysql_cursor.execute(sql_insert_auteur, (row["Auteur :"],))
        mysql_conn.commit()

    # Insertion des sujets
    sql_insert_sujet = "INSERT INTO Sujet (Sujet) VALUES (%s)"
    if pd.notna(row["Sujet :"]):  # V√©rifier si la colonne n'est pas vide
        mysql_cursor.execute(sql_insert_sujet, (row["Sujet :"],))
        mysql_conn.commit()

    # Ins√©rtion des notices correspondantes dans MongoDB
    notices_collection.insert_one({
        "id_Titre": id_titre,
        "Notice_ensemble": row["Notice d'ensemble :"],
        "Notice_catalogue": row["Notice du catalogue :"]
    })

# Fermeture des connexions
mysql_cursor.close()
mysql_conn.close()
mongo_client.close()

print("OK! Donn√©es ins√©r√©es avec succ√®s dans MySQL et MongoDB !")


# IV Requ√™tes READ

sql_query_1 = """
SELECT * FROM Revue 
WHERE Source IN ('Cit√© Internationale Universitaire de Paris', 'Biblioth√®que nationale et universitaire de Strasbourg') 
ORDER BY Titre ASC;
"""
mysql_cursor.execute(sql_query_1)
result_1 = mysql_cursor.fetchall()
print(" Revues des sources universitaires :", result_1)


# 3 Requ√™te SQL pour les revues entre 1939-1945 sans sujet sp√©cifique
sql_query_2 = """
SELECT * FROM Revue 
WHERE Date_d_edition BETWEEN 1939 AND 1960
AND sujet = 'Guerre mondiale (1914-1918) -- Aspect religieux';
;
"""
mysql_cursor.execute(sql_query_2)
result_2 = mysql_cursor.fetchall()
print("Revues entre 1939 et 1960 concernant le sujet 'Guerre mondiale (1914-1918) -- Aspect religieux' :", result_2)

mysql_cursor.close()
mysql_conn.close()



# Connexion √† MongoDB (logiciel):  Requ√™te pour trouver les revues contenant "ISSN" dans "Notice du catalogue"
mongo_client = pymongo.MongoClient("mongodb://localhost:27017/")
mongo_db = mongo_client["revues_mongo"]
notices_collection = mongo_db["Notices"]

doc = notices_collection.find_one()
print(doc)


query = { "Notice_catalogue": { "$regex": "ISSN"} }
result = notices_collection.find(query)

# Affichage des r√©sultats
for doc in result:
    print(doc)

mongo_client.close()