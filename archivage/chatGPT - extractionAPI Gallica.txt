gallica.bnf.fr

Ma requête 1: https://gallica.bnf.fr/SRU?version=1.2&operation=searchRetrieve&query=Le+Figaro

résultat 1:
État HTTP 500 – Internal Server Error
message Could not resolve view with name 'error' in servlet with name 'arkDispatcherServlet'




Ma requête 2:https://gallica.bnf.fr/SRU?version=1.2&operation=searchRetrieve&query=dc.title%20all%20%22Le%20Figaro%22

résultat 2: résultat "riche", mais :
ne contient pas ce que je souhaite 



Prompt pour ma requête 3:

En t'aidant de l'API de recherche de Gallica (https://api.bnf.fr/fr/api-gallica-de-recherche#/) et de l'LAPI Document de Gallica (https://api.bnf.fr/fr/api-document-de-gallica#/Issues). JE veux que tu me proposes une requête API pour extraire au moins un millier d'exemplaires du journal "Le Figaro"depuis sa création.
En effet, a requête que tu m'as proposée précédemment (archRetrieve&query=dc.title%20all%20%22Le%20Figaro%22) contient beaucoup trop peu de résultats, mêle des articles avec d'autres types de résultats (photographies, revues, etc...)


Ma requête 3:
https://gallica.bnf.fr/SRU?version=1.2&operation=searchRetrieve
&query=(dc.title%20all%20%22Le%20Figaro%22)
%20and%20(gallica!type%20all%20%22periodical%22)
%20and%20(dc.language%20all%20%22fre%22)
&maximumRecords=1000


Explication de la requête :
Paramètre	Explication
dc.title all "Le Figaro"	Recherche les documents avec "Le Figaro" dans le titre
gallica!type all "periodical"	Filtre pour ne récupérer que des journaux et périodiques
dc.language all "fre"	Filtre pour récupérer uniquement les documents en français
maximumRecords=1000	Permet de récupérer jusqu’à 1000 résultats en une seule requête



J'ai obtenu le résultat que voici en pièce jointe avec la requête que tu m'as proposée. 

premièrement, je ne comprends pas pourquoi numberOfRecords me donne 18226 résultats, mais je n'observe qu'un nombre bien plus limité d'occurrences quand je fais ctrl+f en recherchant des balises comme dc:publisher (184 occurrences), dc:identifier (190 occurrences, dont d'ailleurs ), etc...

Par ailleurs, j'aimerais, via un notebook jupyter, extraire à partir de ces résultats toutes les informations contenues dans des balises elles-mêmes contenues dans les différentes balises <oai_dc:dc>. Il s'agit donc des balises comme "dc:date", "dc:description", "dc:publisher", etc... 
Informations à partir desquelles je voudrais ensuite :
- agréger mes résultats avec le scrapping de ce même site que j'effectuerai demain (pour lequel je voudrai que tu m'aides aussi).
- nettoyer les outliers et les doublons sur ce résultat aggregé
- créer  2 base de données: une en SQL contenant les données tabulaires issues de mon scrapping et de mon étude d'Api, une autre non-tabulaire (mongo db) contenant des éventuelles descriptions longues et des éventuelles photos numérisées. 
Il faudra vraiment que j'ai quelques milliers de résultats disponibles dans mes bases de données après avoir cumulé le web scraping, la requête d'Api, puis avoir nettoyé mes données.

il faudra par ailleurs que je commit et push tous ces résultats sur github.
propose moi un protocole à suivre étapes par étapes.






